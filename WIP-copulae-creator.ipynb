{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from itertools import combinations \n",
    "from itertools import permutations\n",
    "from itertools import chain\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "courseletlist = (\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F175%2F163757167020201210_Liu_crypto_p2p_lending.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F272%2F1654160257Lesson1-1.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F273%2F1654160288Lesson1-2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F274%2F1654160327Lesson1-3.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F275%2F1654160374Lesson1-4.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F276%2F1654160475Lesson1-5.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F277%2F1654160518Lesson1-6.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F279%2F1654251498Lesson2-1.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F278%2F1654160549Lesson1-7.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F280%2F1654251511Lesson2-2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F281%2F1654251525Lesson2-3.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F103%2F20210303+IA+METIS+Reinforcement+Learning.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F107%2F1636712642CATE_meets_ML_Presentation.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F120%2F163458263420190429+Hae+Ni+LDA+DTM.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F120%2F163646337920210921+Hae+Ni+LDA.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F121%2F163646358220210708+Hae+Ni+LDA+extensions.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F108%2F163595835420210530+METIS+WANG+Kalman+Filter.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F110%2F1632126441nodalida2021_summaryQuality_slides.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F112%2F163664661320211013+Ren+LI+Hae+Expectile+FRM.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F114%2F1635233254Shapley.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F118%2F1636625638FRM%40EM.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F119%2F163231165920210324+Wan+Hae+Li+k-expectile+clustering.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F130%2F1633104997PAC.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F134%2F163368764620210923+Mer+Hae+GAN+Generative+Adversarial+Networks.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F139%2F163402565120211012+Kho+Hae+Trespassing+random+forests+with+a+pointed+stick+for+self+defence.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F217%2F1644582711Berlin_short_course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F235%2F1649426301Variable+importance+measures+for+RF+.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F179%2F16376558632021122+SBA+JW+Hae+EPF++Quantinar.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F186%2F1645194357Presentation_Quantinar_with_videos.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F306%2FBarHan2021_talk.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F181%2F163826827620211130+LI+Hae+Case+based+Bancruptcy+prediction.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F224%2F164728525020220305+LI+Electricity+Market+Coupling.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F196%2F1642599075163458186420200403+METIS+Kho+Hae+Spectral+Clustering+course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F197%2F1642599236164171878820211207+Hae+Zin+Hierarchical+Clustering+course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F198%2F1642599289163231165920210324+Wan+Hae+Li+k-expectile+clustering+course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F200%2F1642686124163527012720210526+SAE+NAG+HAE+SIZ+Understanding+jumps+in+high+frequency+digital+asset+markets_course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F201%2F16426862241636625638FRM%40EM_course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F202%2F1642686343163774999520210912+Hae+Li+Tao+Dynamic+Crypto+Networks_course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F189%2F163958119120211130+Hae+Wan+Kot+ComputerMuseum.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F183%2F1643806658KDE+ill-posed+problems.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F167%2F163699399420211115+Liu+Word+Embeddings+2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F157%2F1642778303introduction_data_science.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F180%2F16377653596.+model+assessment+-+part+4+-+appendix.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F128%2F163707553520210331_METIS_Hel_GANs_for_Time_Series.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F127%2F163458156320190528+Cea+Hae+Scagnostics.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F117%2F163458186420200403+METIS+Kho+Hae+Spectral+Clustering.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F106%2F163458175020200914+Hae+DS2+Data+Science+%26+Digital+Society.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F190%2F1640038524Instruction+for+Creating+Quantlets.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F144%2F1636624210NNCSR_Slides.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F184%2F163888970320211207+Zin+Reu+Hae+USC+Quantinar+40+min+PDF.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F133%2F163707963820210525_Hae_Xia_Crypto_Indices-2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F129%2F163458163120200915+Kim+Hae+Tri+VCRIX.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F123%2F163664570620210922+Mat+Pac+Hae+guide+hedging+CC.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F122%2F163283000220210923+Cul+Hae+Pet+Xia+Cryptocurrency+as+an+asset+class.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F116%2F163774999520210912+Hae+Li+Tao+Dynamic+Crypto+Networks.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F113%2F1632580703FRM+for+Cryptos.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F101%2F163170747120210914+Reu+DSF+Digital+Surrogate+Finance+Doc.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F97%2F163458105020210808+METIS+Win+Pricing+Kernel+Risk+Premium.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F96%2F163299370720210908_CRC21_Hae_Rodeo_or_Ascot.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F96%2F1635155323202109_RoA.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F79%2F164604019720210502+Hae+Har+Reu+Understanding+CCs.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F241%2F1650632942Biographical+Background+Information.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F210%2F164390414020220130+METIS+Gua+Hae+Model+Selection+Criteria.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F194%2F164171878820211207+Hae+Zin+Hierarchical+Clustering.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F170%2F163709451820211117+Hae+Qia+Network+Centrality.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F142%2F163627928020211107+Hae+Iva+Mat+Delaunay+Triangulation_A_Shape.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F135%2F1649084960Chapter+1.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F136%2F1649094328Chapter+2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F137%2F1633950248Chapter+3.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F138%2F1649094430Chapter+4.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = {\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"you're\",\"you've\",\"you'll\",\"you'd\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"she's\",\"her\",\"hers\",\"herself\",\"it\",\"it's\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",'that',\"that'll\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"don't\",\"should\",\"should've\",\"now\",\"d\",\"ll\",\"m\",\"o\",\"re\",\"ve\",\"y\",\"ain\",\"aren\",\"aren't\",\"couldn\",\"couldn't\",\"didn\",\"didn't\",\"doesn\",\"doesn't\",\"hadn\",\"hadn't\",\"hasn\",\"hasn't\",\"haven\",\"haven't\",\"isn\",\"isn't\",\"ma\",\"mightn\",\"mightn't\",\"mustn\",\"mustn't\",\"needn\",\"needn't\",\"shan\",\"shan't\",\"shouldn\",\"shouldn't\",\"wasn\",\"wasn't\",\"weren\",\"weren't\",\"won\",\"won't\",\"wouldn\",\"wouldn't\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ([\"id-{}\".format(i+1) for i in range(len(courseletlist))])\n",
    "# pd.DataFrame({'id_name': ids, 'urlink': courseletlist})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(file_name, url):\n",
    "    '''Download a PDF file with an URL (Step 1)'''\n",
    "    # Define HTTP Headers\n",
    "    headers = {\"User-Agent\": \"Chrome/51.0.2704.103\"}\n",
    "    # Download image\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # if response is OK download the PDF and store it, else write the status\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_string(file_name):\n",
    "    '''Transform a PDF file to a list of string pages (Step 2)'''\n",
    "    # opening the file\n",
    "    imported_pdf = open(file_name, 'rb')\n",
    "    # convert PDF to readable file\n",
    "    transformed_pdf = PyPDF2.PdfFileReader(imported_pdf)\n",
    "    # get number of pages\n",
    "    totalpages = transformed_pdf.numPages\n",
    "    # read the data and store in a list\n",
    "    pdf_output = [transformed_pdf.getPage(i) for i in range(totalpages)]\n",
    "    # extract result\n",
    "    pdf_output = [pdf_output[i].extractText() for i in range(totalpages)]\n",
    "    return pdf_output, totalpages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(file_name):\n",
    "    '''Initial PDF cleaning procedure (Step 3)'''\n",
    "    pdf_output, totalpages = create_string(file_name)\n",
    "# # cleaning URLs\n",
    "    pdf_output = [re.sub(pattern = \"http[^ ]*\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # # cleaning symbols\n",
    "    pdf_output = [re.sub(pattern = \"\\\\n\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \"\\W|\\d\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # pdf_output = [re.sub(pattern = \"ðœƒ|ð”¼|ð–¤|Î¸|Ïˆ|ð’³|â„‹|ð’«|ð’œ|ð’¢|ð’Ÿ|ð’ž|è°¢|ç¤¾|ä¼š|åŽ†|å²|åš|ç‰©|é¦†|æˆ|éƒ½|å·ž|å­¸|è€…|å­¦|å¤ª|æ¹–|ä¹‹|å…‰|â„±|â„¤|â„|Ï„|â„›|â„™|â„•|â„“|Î²|â„‹|Îµ|Ù‡ÙˆÚ©|Ø±ÙˆÙ†|Ïµ|Î»|Ï•|Ï‰|Î±|Ïƒ|Ï€|Î¾|ÏŒ|Ï‚|Î¼|â„’|Î´|Î³|åŽ¦|Ïˆ|Ïƒ|Ï|Î½|Î¸|Î·\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \"[^a-zA-Z]\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # # cleaning multispaces\n",
    "    pdf_output = [re.sub(pattern = \"\\s{2,}\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # # cleaning out 1-2-worders\n",
    "    pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # # lower-casing\n",
    "    pdf_output = [pdf_output[i].lower() for i in range(totalpages)]\n",
    "    return pdf_output, totalpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lists(file_name):\n",
    "    '''Creating the base one-word, two-word and three-word lists, the permutation lists for two- and three-word lists (Step 4)'''\n",
    "    pdf_output, totalpages = cleaning(file_name)\n",
    "    # split to a list\n",
    "    word_list = [pdf_output[i].split(\" \") for i in range(totalpages)]\n",
    "    # stemming\n",
    "    word_list_stemmed = [[ps.stem(word_list[i][j]) for j in range(len(word_list[i]))] for i in range(totalpages)]    \n",
    "    word_list_stemmed = pd.DataFrame(word_list_stemmed)\n",
    "    # one-word section\n",
    "    one_word_list = [word_list_stemmed.iloc[j, i] for j in range(totalpages) for i in range(len(word_list_stemmed))]\n",
    "    # one_word_list = [x for x in one_word_list if len(x) > 2] # problem with nulls\n",
    "    one_word_list = [x for x in one_word_list if x not in stopwords_list]\n",
    "\n",
    "    # two-word section\n",
    "    two_word_list = [[word_list_stemmed.iloc[j, i], word_list_stemmed.iloc[j, i+1]] for j in range(totalpages)  for i in range(len(word_list_stemmed) - 1)]\n",
    "    two_word_permutation_list = [[p for p in permutations(two_word_list[k])][1:] for k in range(len(two_word_list))]\n",
    "    two_word_permutation_set = set(list(chain(*two_word_permutation_list)))\n",
    "    two_word_permutation_set = pd.DataFrame(two_word_permutation_set)\n",
    "\n",
    "    # three-word section\n",
    "    three_word_list = [[word_list_stemmed.iloc[j, i], word_list_stemmed.iloc[j, i+1], word_list_stemmed.iloc[j, i+2]] for j in range(totalpages) for i in range(len(word_list_stemmed) - 2)]\n",
    "    three_word_permutation_list = [[p for p in permutations(three_word_list[k])][1:] for k in range(len(three_word_list))]\n",
    "    three_word_permutation_set = set(list(chain(*three_word_permutation_list)))\n",
    "    three_word_permutation_set = pd.DataFrame(three_word_permutation_set)\n",
    "\n",
    "    return word_list_stemmed, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurrance_three_matrix_creator(file_name):\n",
    "    '''Creating the occurrance matrices for the three-word lists (Step 5)'''\n",
    "    word_list_stemmed, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set, pdf_output, totalpages = word_lists(file_name)\n",
    "    # copying the data\n",
    "    words = three_word_list.copy()\n",
    "    # converting to a dataframe\n",
    "    words = pd.DataFrame(three_word_list)\n",
    "    # creating the three-word combinations as one string\n",
    "    words = [words.iloc[i,0] + \" \" + words.iloc[i,1] + \" \" + words.iloc[i,2] for i in range(len(three_word_list)) if words.iloc[i,].isna().any() == False]\n",
    "    # crating the dictionary\n",
    "    dictionary_three_word = dict()\n",
    "    \n",
    "    # counting word occurances\n",
    "    for word in words:\n",
    "        if word in dictionary_three_word:\n",
    "            dictionary_three_word[word] = dictionary_three_word[word] + 1\n",
    "        else:\n",
    "            dictionary_three_word[word] = 1\n",
    "    \n",
    "    # creating the occurance matrix\n",
    "    dictionary_three_words = dictionary_three_word.items()\n",
    "    dictionary_three_list = list(dictionary_three_words)\n",
    "    occurrence_three_matrix = pd.DataFrame(dictionary_three_list)\n",
    "    occurrence_three_matrix = occurrence_three_matrix.rename(columns={0:\"word\", 1:\"occurance\"})\n",
    "    \n",
    "    # clean of NaNs\n",
    "    occurrence_three_matrix = occurrence_three_matrix.loc[occurrence_three_matrix.word.isna() == False, ]\n",
    "    occurrence_three_matrix = occurrence_three_matrix.loc[occurrence_three_matrix.word != \"None\", ]\n",
    "\n",
    "    # sort values\n",
    "    occurrence_three_matrix = occurrence_three_matrix.sort_values(\"occurance\", ascending=False)\n",
    "\n",
    "    # re-indexing\n",
    "    occurrence_three_matrix['index'] = range(len(occurrence_three_matrix))\n",
    "    occurrence_three_matrix = occurrence_three_matrix.set_index('index')\n",
    "    return occurrence_three_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurrance_two_matrix_creator(file_name):\n",
    "    '''Creating the occurrance matrices for the two-word lists (Step 6)'''\n",
    "    word_list_stemmed, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set = word_lists(file_name)\n",
    "    # copying the data\n",
    "    words = two_word_list.copy()\n",
    "    # converting to a dataframe\n",
    "    words = pd.DataFrame(two_word_list)\n",
    "    # creating the three-word combinations as one string\n",
    "    words = [words.iloc[i,0] + \" \" + words.iloc[i,1] for i in range(len(two_word_list)) if words.iloc[i,].isna().any() == False]\n",
    "    # crating the dictionary\n",
    "    dictionary_two_word = dict()\n",
    "    \n",
    "    # counting word occurances\n",
    "    for word in words:\n",
    "        if word in dictionary_two_word:\n",
    "            dictionary_two_word[word] = dictionary_two_word[word] + 1\n",
    "        else:\n",
    "            dictionary_two_word[word] = 1\n",
    "    \n",
    "    # creating the occurance matrix\n",
    "    dictionary_two_words = dictionary_two_word.items()\n",
    "    dictionary_three_list = list(dictionary_two_words)\n",
    "    occurrence_two_matrix = pd.DataFrame(dictionary_three_list)\n",
    "    occurrence_two_matrix = occurrence_two_matrix.rename(columns={0:\"word\", 1:\"occurance\"})\n",
    "    \n",
    "    # clean of NaNs\n",
    "    occurrence_two_matrix = occurrence_two_matrix.loc[occurrence_two_matrix.word.isna() == False, ]\n",
    "    occurrence_two_matrix = occurrence_two_matrix.loc[occurrence_two_matrix.word != \"None\", ]\n",
    "\n",
    "    # sort values\n",
    "    occurrence_two_matrix = occurrence_two_matrix.sort_values(\"occurance\", ascending=False)\n",
    "\n",
    "    # re-indexing\n",
    "    occurrence_two_matrix['index'] = range(len(occurrence_two_matrix))\n",
    "    occurrence_two_matrix = occurrence_two_matrix.set_index('index')\n",
    "    return occurrence_two_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurance_one_matrix_creator(file_name):\n",
    "    '''Creating the occurrance matrix for one-word combinations (Step 7)'''\n",
    "    word_list_stemmed, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set = word_lists(file_name)\n",
    "    # copying the data\n",
    "    words = one_word_list.copy()\n",
    "    # creating the three-word combinations as one string\n",
    "    words = [x for x in words if x != \"\"]\n",
    "    words = [x for x in words if x != \" \"]    \n",
    "    # crating the dictionary\n",
    "    dictionary_one_word = dict()\n",
    "    \n",
    "    # counting word occurances\n",
    "    for word in words:\n",
    "        if word in dictionary_one_word:\n",
    "            dictionary_one_word[word] = dictionary_one_word[word] + 1\n",
    "        else:\n",
    "            dictionary_one_word[word] = 1\n",
    "    \n",
    "    # creating the occurance matrix\n",
    "    dictionary_one_word = dictionary_one_word.items()\n",
    "    occurrence_one_matrix = pd.DataFrame(dictionary_one_word)\n",
    "    occurrence_one_matrix = occurrence_one_matrix.rename(columns={0:\"word\", 1:\"occurance\"})\n",
    "    \n",
    "    # clean of NaNs\n",
    "    occurrence_one_matrix = occurrence_one_matrix.loc[occurrence_one_matrix.word.isna() == False, ]\n",
    "    occurrence_one_matrix = occurrence_one_matrix.loc[occurrence_one_matrix.word != \"None\", ]\n",
    "\n",
    "    # sort values\n",
    "    occurrence_one_matrix = occurrence_one_matrix.sort_values(\"occurance\", ascending=False)\n",
    "\n",
    "    # re-indexing\n",
    "    occurrence_one_matrix['index'] = range(len(occurrence_one_matrix))\n",
    "    occurrence_one_matrix = occurrence_one_matrix.set_index('index')\n",
    "    return occurrence_one_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_script(file_name, url):\n",
    "    download_pdf(file_name, url)\n",
    "    word_lists(file_name)\n",
    "    word_list_stemmed, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set = word_lists(file_name)\n",
    "    occurrence_three_matrix = occurrance_three_matrix_creator(file_name)\n",
    "    occurrence_two_matrix = occurrance_two_matrix_creator(file_name)\n",
    "    occurrence_one_matrix = occurance_one_matrix_creator(file_name)\n",
    "    os.remove(file_name)\n",
    "    occurrence_one_matrix['id'] = file_name\n",
    "    occurrence_two_matrix['id'] = file_name\n",
    "    occurrence_three_matrix['id'] = file_name\n",
    "    two_word_permutation_set['id'] = file_name\n",
    "    three_word_permutation_set['id'] = file_name\n",
    "    return occurrence_one_matrix, occurrence_two_matrix, occurrence_three_matrix, two_word_permutation_set, three_word_permutation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence_one_matrix_z, occurrence_two_matrix_z, occurrence_three_matrix_z, two_word_permutation_set_z, three_word_permutation_set_z = main_script(ids[0], courseletlist[0])\n",
    "for i in range(1, 33):\n",
    "    occurrence_one_matrix, occurrence_two_matrix, occurrence_three_matrix, two_word_permutation_set, three_word_permutation_set = main_script(ids[i], courseletlist[i])\n",
    "    occurrence_one_matrix_z = pd.concat([occurrence_one_matrix_z, occurrence_one_matrix])\n",
    "    occurrence_two_matrix_z = pd.concat([occurrence_two_matrix_z, occurrence_two_matrix])\n",
    "    occurrence_three_matrix_z = pd.concat([occurrence_three_matrix_z, occurrence_three_matrix])\n",
    "    two_word_permutation_set_z = pd.concat([two_word_permutation_set_z, two_word_permutation_set])\n",
    "    three_word_permutation_set_z = pd.concat([three_word_permutation_set_z, three_word_permutation_set])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 34, 35 are not working, 40-45 error, 61, 62 63\n",
    "# 33, 35, 61, 62, 63, 64, 67, 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence_one_matrix, occurrence_two_matrix, occurrence_three_matrix, two_word_permutation_set, three_word_permutation_set = main_script(ids[34], courseletlist[34])\n",
    "occurrence_one_matrix_z = pd.concat([occurrence_one_matrix_z, occurrence_one_matrix])\n",
    "occurrence_two_matrix_z = pd.concat([occurrence_two_matrix_z, occurrence_two_matrix])\n",
    "occurrence_three_matrix_z = pd.concat([occurrence_three_matrix_z, occurrence_three_matrix])\n",
    "two_word_permutation_set_z = pd.concat([two_word_permutation_set_z, two_word_permutation_set])\n",
    "three_word_permutation_set_z = pd.concat([three_word_permutation_set_z, three_word_permutation_set])\n",
    "print(34)\n",
    "for i in range(36, 61):\n",
    "    occurrence_one_matrix, occurrence_two_matrix, occurrence_three_matrix, two_word_permutation_set, three_word_permutation_set = main_script(ids[i], courseletlist[i])\n",
    "    occurrence_one_matrix_z = pd.concat([occurrence_one_matrix_z, occurrence_one_matrix])\n",
    "    occurrence_two_matrix_z = pd.concat([occurrence_two_matrix_z, occurrence_two_matrix])\n",
    "    occurrence_three_matrix_z = pd.concat([occurrence_three_matrix_z, occurrence_three_matrix])\n",
    "    two_word_permutation_set_z = pd.concat([two_word_permutation_set_z, two_word_permutation_set])\n",
    "    three_word_permutation_set_z = pd.concat([three_word_permutation_set_z, three_word_permutation_set])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "for i in range(65, 67):\n",
    "    occurrence_one_matrix, occurrence_two_matrix, occurrence_three_matrix, two_word_permutation_set, three_word_permutation_set = main_script(ids[i], courseletlist[i])\n",
    "    occurrence_one_matrix_z = pd.concat([occurrence_one_matrix_z, occurrence_one_matrix])\n",
    "    occurrence_two_matrix_z = pd.concat([occurrence_two_matrix_z, occurrence_two_matrix])\n",
    "    occurrence_three_matrix_z = pd.concat([occurrence_three_matrix_z, occurrence_three_matrix])\n",
    "    two_word_permutation_set_z = pd.concat([two_word_permutation_set_z, two_word_permutation_set])\n",
    "    three_word_permutation_set_z = pd.concat([three_word_permutation_set_z, three_word_permutation_set])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "for i in range(69, 70):\n",
    "    occurrence_one_matrix, occurrence_two_matrix, occurrence_three_matrix, two_word_permutation_set, three_word_permutation_set = main_script(ids[i], courseletlist[i])\n",
    "    occurrence_one_matrix_z = pd.concat([occurrence_one_matrix_z, occurrence_one_matrix])\n",
    "    occurrence_two_matrix_z = pd.concat([occurrence_two_matrix_z, occurrence_two_matrix])\n",
    "    occurrence_three_matrix_z = pd.concat([occurrence_three_matrix_z, occurrence_three_matrix])\n",
    "    two_word_permutation_set_z = pd.concat([two_word_permutation_set_z, two_word_permutation_set])\n",
    "    three_word_permutation_set_z = pd.concat([three_word_permutation_set_z, three_word_permutation_set])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>id-1</th>\n",
       "      <th>id-10</th>\n",
       "      <th>id-11</th>\n",
       "      <th>id-12</th>\n",
       "      <th>id-13</th>\n",
       "      <th>id-14</th>\n",
       "      <th>id-15</th>\n",
       "      <th>id-16</th>\n",
       "      <th>id-17</th>\n",
       "      <th>id-18</th>\n",
       "      <th>...</th>\n",
       "      <th>id-59</th>\n",
       "      <th>id-6</th>\n",
       "      <th>id-60</th>\n",
       "      <th>id-61</th>\n",
       "      <th>id-66</th>\n",
       "      <th>id-67</th>\n",
       "      <th>id-7</th>\n",
       "      <th>id-70</th>\n",
       "      <th>id-8</th>\n",
       "      <th>id-9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaab</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaac</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaackxicbvdlsgmxfm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaacunicbvlpa</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaacwxicbvfdsymxfm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zvi</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zvt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zytdnt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8023 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id                  id-1  id-10  id-11  id-12  id-13  id-14  id-15  id-16  \\\n",
       "word                                                                        \n",
       "aaab                 NaN    NaN    NaN    NaN    NaN    2.0    1.0    2.0   \n",
       "aaac                 NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "aaackxicbvdlsgmxfm   NaN    NaN    NaN    NaN    NaN    NaN    1.0    NaN   \n",
       "aaacunicbvlpa        NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "aaacwxicbvfdsymxfm   NaN    NaN    NaN    NaN    NaN    NaN    1.0    NaN   \n",
       "...                  ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "zvi                  NaN    NaN    NaN    NaN    NaN    NaN    NaN    1.0   \n",
       "zvt                  NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "zyt                  NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "zytdnt               NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "zzt                  NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "id                  id-17  id-18  ...  id-59  id-6  id-60  id-61  id-66  \\\n",
       "word                              ...                                     \n",
       "aaab                  NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "aaac                  NaN    1.0  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "aaackxicbvdlsgmxfm    NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "aaacunicbvlpa         NaN    1.0  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "aaacwxicbvfdsymxfm    NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "...                   ...    ...  ...    ...   ...    ...    ...    ...   \n",
       "zvi                   NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "zvt                   NaN    NaN  ...    1.0   NaN    NaN    NaN    NaN   \n",
       "zyt                   NaN    NaN  ...    1.0   NaN    NaN    NaN    NaN   \n",
       "zytdnt                NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "zzt                   NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "\n",
       "id                  id-67  id-7  id-70  id-8  id-9  \n",
       "word                                                \n",
       "aaab                  NaN   NaN    NaN   NaN   NaN  \n",
       "aaac                  NaN   NaN    NaN   NaN   NaN  \n",
       "aaackxicbvdlsgmxfm    NaN   NaN    NaN   NaN   NaN  \n",
       "aaacunicbvlpa         NaN   NaN    NaN   NaN   NaN  \n",
       "aaacwxicbvfdsymxfm    NaN   NaN    NaN   NaN   NaN  \n",
       "...                   ...   ...    ...   ...   ...  \n",
       "zvi                   NaN   NaN    NaN   NaN   NaN  \n",
       "zvt                   NaN   NaN    NaN   NaN   NaN  \n",
       "zyt                   NaN   NaN    NaN   NaN   NaN  \n",
       "zytdnt                NaN   NaN    NaN   NaN   NaN  \n",
       "zzt                   NaN   NaN    NaN   NaN   NaN  \n",
       "\n",
       "[8023 rows x 62 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_table_one = pd.pivot_table(occurrence_one_matrix_z, index=['word'], columns=['id'], values='occurance', aggfunc=sum)\n",
    "lsa_table_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>id-1</th>\n",
       "      <th>id-10</th>\n",
       "      <th>id-11</th>\n",
       "      <th>id-12</th>\n",
       "      <th>id-13</th>\n",
       "      <th>id-14</th>\n",
       "      <th>id-15</th>\n",
       "      <th>id-16</th>\n",
       "      <th>id-17</th>\n",
       "      <th>id-18</th>\n",
       "      <th>...</th>\n",
       "      <th>id-59</th>\n",
       "      <th>id-6</th>\n",
       "      <th>id-60</th>\n",
       "      <th>id-61</th>\n",
       "      <th>id-66</th>\n",
       "      <th>id-67</th>\n",
       "      <th>id-7</th>\n",
       "      <th>id-70</th>\n",
       "      <th>id-8</th>\n",
       "      <th>id-9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accur</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appendix</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appendixrefer</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zvi michal</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zvt jzvt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyt zvt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zytdnt motiv</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzt forget</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36929 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id              id-1  id-10  id-11  id-12  id-13  id-14  id-15  id-16  id-17  \\\n",
       "word                                                                           \n",
       " accur           NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       " algorithm       NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    7.0   \n",
       " alpha           NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       " appendix        2.0    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       " appendixrefer   NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...              ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "zvi michal       NaN    NaN    NaN    NaN    NaN    NaN    NaN    1.0    NaN   \n",
       "zvt jzvt         NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "zyt zvt          NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "zytdnt motiv     NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "zzt forget       NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "id              id-18  ...  id-59  id-6  id-60  id-61  id-66  id-67  id-7  \\\n",
       "word                   ...                                                  \n",
       " accur            NaN  ...    NaN   NaN    NaN    NaN    NaN    NaN   NaN   \n",
       " algorithm        NaN  ...    NaN   NaN    NaN    NaN    NaN    NaN   NaN   \n",
       " alpha            NaN  ...    NaN   NaN    NaN    NaN    1.0    NaN   NaN   \n",
       " appendix         NaN  ...    NaN   NaN    NaN    NaN    NaN    NaN   NaN   \n",
       " appendixrefer    NaN  ...    NaN   NaN    NaN    NaN    NaN    NaN   NaN   \n",
       "...               ...  ...    ...   ...    ...    ...    ...    ...   ...   \n",
       "zvi michal        NaN  ...    NaN   NaN    NaN    NaN    NaN    NaN   NaN   \n",
       "zvt jzvt          NaN  ...    1.0   NaN    NaN    NaN    NaN    NaN   NaN   \n",
       "zyt zvt           NaN  ...    1.0   NaN    NaN    NaN    NaN    NaN   NaN   \n",
       "zytdnt motiv      NaN  ...    NaN   NaN    NaN    NaN    NaN    NaN   NaN   \n",
       "zzt forget        NaN  ...    NaN   NaN    NaN    NaN    NaN    NaN   NaN   \n",
       "\n",
       "id              id-70  id-8  id-9  \n",
       "word                               \n",
       " accur            NaN   NaN   NaN  \n",
       " algorithm        NaN   NaN   NaN  \n",
       " alpha            NaN   NaN   NaN  \n",
       " appendix         NaN   NaN   NaN  \n",
       " appendixrefer    NaN   NaN   NaN  \n",
       "...               ...   ...   ...  \n",
       "zvi michal        NaN   NaN   NaN  \n",
       "zvt jzvt          NaN   NaN   NaN  \n",
       "zyt zvt           NaN   NaN   NaN  \n",
       "zytdnt motiv      NaN   NaN   NaN  \n",
       "zzt forget        NaN   NaN   NaN  \n",
       "\n",
       "[36929 rows x 62 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_table_two = pd.pivot_table(occurrence_two_matrix_z, index=['word'], columns=['id'], values='occurance', aggfunc=sum)\n",
    "lsa_table_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>id-1</th>\n",
       "      <th>id-10</th>\n",
       "      <th>id-11</th>\n",
       "      <th>id-12</th>\n",
       "      <th>id-13</th>\n",
       "      <th>id-14</th>\n",
       "      <th>id-15</th>\n",
       "      <th>id-16</th>\n",
       "      <th>id-17</th>\n",
       "      <th>id-18</th>\n",
       "      <th>...</th>\n",
       "      <th>id-59</th>\n",
       "      <th>id-6</th>\n",
       "      <th>id-60</th>\n",
       "      <th>id-61</th>\n",
       "      <th>id-66</th>\n",
       "      <th>id-67</th>\n",
       "      <th>id-7</th>\n",
       "      <th>id-70</th>\n",
       "      <th>id-8</th>\n",
       "      <th>id-9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accur represent</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm introduct</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha shape</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appendix back</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appendix crypto</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zvi michal the</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zvt jzvt rodeo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyt zvt jzvt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zytdnt motiv valuat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzt forget gate</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46253 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id                    id-1  id-10  id-11  id-12  id-13  id-14  id-15  id-16  \\\n",
       "word                                                                          \n",
       " accur represent       NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       " algorithm introduct   NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       " alpha shape           NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       " appendix back         NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       " appendix crypto       2.0    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "zvi michal the         NaN    NaN    NaN    NaN    NaN    NaN    NaN    1.0   \n",
       "zvt jzvt rodeo         NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "zyt zvt jzvt           NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "zytdnt motiv valuat    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "zzt forget gate        NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "id                    id-17  id-18  ...  id-59  id-6  id-60  id-61  id-66  \\\n",
       "word                                ...                                     \n",
       " accur represent        NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       " algorithm introduct    7.0    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       " alpha shape            NaN    NaN  ...    NaN   NaN    NaN    NaN    1.0   \n",
       " appendix back          NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       " appendix crypto        NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "...                     ...    ...  ...    ...   ...    ...    ...    ...   \n",
       "zvi michal the          NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "zvt jzvt rodeo          NaN    NaN  ...    1.0   NaN    NaN    NaN    NaN   \n",
       "zyt zvt jzvt            NaN    NaN  ...    1.0   NaN    NaN    NaN    NaN   \n",
       "zytdnt motiv valuat     NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "zzt forget gate         NaN    NaN  ...    NaN   NaN    NaN    NaN    NaN   \n",
       "\n",
       "id                    id-67  id-7  id-70  id-8  id-9  \n",
       "word                                                  \n",
       " accur represent        NaN   NaN    NaN   NaN   NaN  \n",
       " algorithm introduct    NaN   NaN    NaN   NaN   NaN  \n",
       " alpha shape            NaN   NaN    NaN   NaN   NaN  \n",
       " appendix back          NaN   NaN    NaN   NaN   NaN  \n",
       " appendix crypto        NaN   NaN    NaN   NaN   NaN  \n",
       "...                     ...   ...    ...   ...   ...  \n",
       "zvi michal the          NaN   NaN    NaN   NaN   NaN  \n",
       "zvt jzvt rodeo          NaN   NaN    NaN   NaN   NaN  \n",
       "zyt zvt jzvt            NaN   NaN    NaN   NaN   NaN  \n",
       "zytdnt motiv valuat     NaN   NaN    NaN   NaN   NaN  \n",
       "zzt forget gate         NaN   NaN    NaN   NaN   NaN  \n",
       "\n",
       "[46253 rows x 62 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_table_three = pd.pivot_table(occurrence_three_matrix_z, index=['word'], columns=['id'], values='occurance', aggfunc=sum)\n",
    "lsa_table_three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] inplement stemming\n",
    "- [x] get rid of the greek letters (to the end)\n",
    "- [x] get rid of chineese letters (to the end)\n",
    "- [ ] do LSM \n",
    "- [ ] create a script to take in request words\n",
    "- [ ] apply lemmatization to them\n",
    "- [ ] get first search results \n",
    "- [ ] draw first word-grams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_table_one = lsa_table_one.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsa_table_one = np.log(lsa_table_one+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 100.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "data = lsa_table_one.transpose()\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy = False))\n",
    "X_lsa = lsa.fit_transform(data)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
